{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "check the smart idea here. https://math.stackexchange.com/questions/1945329/can-you-transpose-a-matrix-using-matrix-multiplication\n",
    "The only problem is that this method takes way too much memory and perhaps we need to figure out a better way to make transpose operation trainable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4398046511104 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 49>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     47\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitialization passed!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 49\u001B[0m \u001B[43mtest_trainable_sparse_transpose_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36mtest_trainable_sparse_transpose_layer\u001B[1;34m()\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtest_trainable_sparse_transpose_layer\u001B[39m():\n\u001B[0;32m     30\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m\n\u001B[1;32m---> 31\u001B[0m     layer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainableSparseTransposeLayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# Create a random tensor of size n x n\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     A \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(n, n)\n",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36mTrainableSparseTransposeLayer.__init__\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m     17\u001B[0m size \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mSize([n \u001B[38;5;241m*\u001B[39m n, n \u001B[38;5;241m*\u001B[39m n])\n\u001B[0;32m     18\u001B[0m initial_sparse_T \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msparse_coo_tensor(indices, values, size)\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mParameter(\u001B[43minitial_sparse_T\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_dense\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mParameter(torch\u001B[38;5;241m.\u001B[39mzeros(n \u001B[38;5;241m*\u001B[39m n, \u001B[38;5;241m1\u001B[39m), requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4398046511104 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TrainableTensorTransposeLayer(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(TrainableTensorTransposeLayer, self).__init__()\n",
    "\n",
    "        # Create the initial tensor T close to the transpose operation for n x n matrices\n",
    "        I = torch.eye(n)\n",
    "        initial_T = torch.zeros(n * n, n * n)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                row_idx = i * n + j\n",
    "                col_idx = j * n + i\n",
    "                initial_T[row_idx, col_idx] = 1\n",
    "\n",
    "        self.weights = nn.Parameter(initial_T, requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(n * n, 1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_reshaped = x.reshape(-1, 1)  # Reshape to column vector\n",
    "        out_reshaped = torch.mm(self.weights, x_reshaped) + self.bias\n",
    "        return out_reshaped.reshape(x.size())  # Reshape back to n x n\n",
    "\n",
    "# Function to test the layer\n",
    "def test_trainable_tensor_transpose_layer():\n",
    "    n = 1024\n",
    "    layer = TrainableTensorTransposeLayer(n)\n",
    "\n",
    "    # Create a random tensor of size n x n\n",
    "    A = torch.rand(n, n)\n",
    "\n",
    "    # Check initially if it's performing approximately a transpose operation\n",
    "    output = layer(A)\n",
    "    expected = A.t()\n",
    "\n",
    "    diff = torch.abs(output - expected)\n",
    "    max_diff = torch.max(diff).item()\n",
    "    mean_diff = torch.mean(diff).item()\n",
    "\n",
    "    if not torch.allclose(output, expected, atol=1e-2):\n",
    "        print(f\"Initialization discrepancy detected! Max difference: {max_diff}, Mean difference: {mean_diff}\")\n",
    "    else:\n",
    "        print(\"Initialization passed!\")\n",
    "\n",
    "    # Additional tests or training can be added here\n",
    "\n",
    "test_trainable_tensor_transpose_layer()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A workaround method should be maintaining a K_t layer inside transformer other than K layer, so we want the layer to directly learn K_t other than K because we will need it later anyway. But there might be problems, we need to figure them out."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(TransformerAttention, self).__init__()\n",
    "\n",
    "        # Linear layers for Q and K-transpose\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.key_transpose_layer = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "\n",
    "        # Get K-transpose directly\n",
    "        K_T = self.key_transpose_layer(x)\n",
    "\n",
    "        # Attention calculation\n",
    "        attention_scores = torch.matmul(Q, K_T)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "# Test\n",
    "embed_dim = 512\n",
    "model = TransformerAttention(embed_dim)\n",
    "input_embeddings = torch.rand(10, embed_dim)  # batch of 10 sequences\n",
    "output = model(input_embeddings)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
